import os
import json
import joblib
import numpy as np
import pandas as pd
from tpot import TPOTRegressor
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from aiflib.data_manager import DataManager
from aiflib.config import Config
from aiflib.logger import Logger, UiPathUsageException

# Constants
_UNTRAINED_HELP = """\n This TPOT Python Automated Machine Learning Pipeline 
has not been trained. Use AI Fabric to train this model on your own tabular data.

The model will read all csv files in the directory recursively. Each file must have 
a header (the first line of the csv file) with feature column names and a target 
column name. The name of the target column must specified using the [target_column] 
environment variable.

TPOT will automate the most tedious part of machine learning by intelligently 
exploring thousands of possible pipelines to find the best one for your data.

Once TPOT is finished searching, it provides you with the Python code for the 
best pipeline it found so you can tinker with the pipeline from there. See AI Fabric 
Documentation for a detailed explanation and other configuration parameters.
"""

class Model():
    def __init__(self, is_infer_only = False):
        self.config = Config()
        self.logger = Logger(__name__)
        self._model = self.load_model()
        self._label_encoder = self.load_labelencoder()

    def train(self, directory):

        dm = DataManager(directory)
        
        if not dm.validate():
            raise UiPathUsageException("No valid data to run this pipeline.")

        data_df = dm.get_data()
        X = data_df[dm.get_feature_columns()].values
        y = data_df[dm.get_target_column()].values

        help_string = "Warning: You have retrained a model which was generated by a TPOT optimization pipeline.\
        \nFor optimal results please run the TPOT optimization pipeline from scratch by training package version [1.0]."

        if not self.is_trained() or self.config.warm_start == True:
            self._model = self.build_model(X, y)
        else:
            self._model.fit(X, y)
            self.logger.info(f"Finished retraining model.")
            self.logger.info(help_string)
            
        joblib.dump(self._model, os.path.join(self.config.cur_dir, "model", "Model.sav"))
    

    def evaluate(self, evaluation_directory):

        dm = DataManager(evaluation_directory)
        data_df = dm.get_data()

        if not dm.validate(for_train = False):
            self.logger.info("No valid test data to run this evaluation pipeline.")

        data_df = dm.get_data()
        X = data_df[dm.get_feature_columns()].values
        y = data_df[dm.get_target_column()].values

        if not self.is_trained():
            self.logger.info(_UNTRAINED_HELP)
        else:
            score = self._model.score(X, y)
            self.logger.info(f"Evaluation score = {score}")
            return score

    def process_data(self, directory):

        if not self.config.test_data_from_ui:
            dm = DataManager(directory)
            
            if not dm.validate():
                raise UiPathUsageException("No valid data to run this pipeline.")
            
            all_data = dm.get_data()

            # Stratified split
            percentage = self.config.process_data_split_percentage
            
            train, test = train_test_split(
                all_data, 
                test_size = percentage, 
                random_state = self.config.seed, 
                )

            # Write train.csv
            DataManager.write_dataframe(
                train, 'train', self.config.train_data_directory)

            # Write evaluate.csv
            DataManager.write_dataframe(
                test, 'test', self.config.test_data_directory)
        else:
            dm = DataManager(directory)

            if not dm.validate():
                raise UiPathUsageException("No valid data to run this pipeline.")
            
            all_data = dm.get_data()

            # Write train.csv
            DataManager.write_dataframe(
                all_data, 'train', self.config.train_data_directory)            
            self.logger.info("Did not split data into train and test sets. Model will be evaluated on data selected from UI.")


    def build_model(self, X, y):
        # Perform missing value imputation as scikit-learn models can't handle NaN's
        nan_imputer = SimpleImputer(missing_values=np.nan, strategy="mean")
        X = nan_imputer.fit_transform(X)

        pipeline_optimizer = TPOTRegressor(
            generations = self.config.generations, 
            population_size = self.config.population_size,
            offspring_size = self.config.offspring_size,
            mutation_rate = self.config.mutation_rate,
            crossover_rate = self.config.crossover_rate,
            scoring = self.config.scoring, 
            cv = self.config.cv,
            subsample = self.config.subsample, 
            n_jobs = -1,
            max_time_mins = self.config.max_time_mins, 
            max_eval_time_mins = self.config.max_eval_time_mins,
            random_state = self.config.seed, 
            config_dict = self.config.classifier_config_dict,
            warm_start = self.config.warm_start,
            memory = self.config.artifacts_directory,
            verbosity = 1
            )
        
        # Fit TPOT to data
        pipeline_optimizer.fit(X, y)
        self.logger.info(f"Finished running TPOT optimization pipeline.")

        # Export fitted pipeline to artifacts directory
        pipeline_path = os.path.join(self.config.artifacts_directory, "TPOT_pipeline.py")
        pipeline_optimizer.export(pipeline_path)
        self.logger.info(f"Saving best pipeline to {pipeline_path}")

        # Create new pipeline which contains nan_imputer
        pipe = Pipeline(
            [
                ("nan_imputer", nan_imputer),
                ("tpot_pipeline", pipeline_optimizer.fitted_pipeline_),
            ]
        )
        return pipe

    def predict(self, mlskill_input): 

        data = pd.read_json(mlskill_input)
        predictions = self._model.predict(data.values)
        return json.dumps(predictions.tolist())

    def load_model(self):
        if os.path.isfile(os.path.join(self.config.cur_dir, "model", "Model.sav")):
            self.logger.info(f"Loading pre-trained model...")
            return joblib.load(os.path.join(self.config.cur_dir, "model", "Model.sav"))
        else:
            return None

    def load_labelencoder(self):
        if os.path.isfile(os.path.join(self.config.cur_dir, "model", "LabelEncoder.sav")):
            self.logger.info(f"Loading label encoder...")
            return joblib.load(os.path.join(self.config.cur_dir, "model", "LabelEncoder.sav"))
        else:
            return None

    def is_trained(self):
        if self._model is None:
            return False
        else:
            return True